---
title: "Airline passengers satisfaction analysis"
author: "Süleyman Erim, Giacomo Schiavo, Mattia Varagnolo"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction to data

This section introduces the purpose of the exploratory data analysis
(EDA) and sets up the necessary libraries and data files.

```{r message=FALSE}
# import libraries
library(tidyverse)
library(tinytex)
library(dplyr)
library(corrplot)
library(ggplot2)
library(gridExtra)
library(correlation)
library(reshape)
library(reshape2)
library(tidyverse) # for data manipulation
library(ggplot2) # for plotting
library(gridExtra) # for grid.arrange
library(regclass) # for VIF package
library(MLmetrics) # to create confusion matrix
library(pROC) # for ROC Curve
library(e1071) # for Naive Bayes Classifier
library(class)
library(caret)
library(corrr)
library(ppcor)
library(glmnet) # for Lasso Regression
```

```{r message=FALSE}
data_train = read.csv("train.csv")
data_test = read.csv("test.csv")


# merge train and test data
data = rbind(data_train, data_test)
attach(data)
```

# Data Analysis

In this project, we will develop a predictive model to determine whether
a passenger will be satisfied or dissatisfied with the services offered
by an airline company. The dataset used for this project is a survey on
airline passenger satisfaction, which contains information about
passengers' demographics, travel preferences, and satisfaction with
various aspects of their flights.

The dataset:
<https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction>

## Description of our variables

Here are the variables in the dataset:

-   Gender: Gender of the passengers (Female, Male)
-   Customer Type: The customer type (Loyal customer, disloyal customer)
-   Age: The actual age of the passengers
-   Type of Travel: Purpose of the flight of the passengers (Personal
    Travel, Business Travel)
-   Class: Travel class in the plane of the passengers (Business, Eco,
    Eco Plus)
-   Flight distance: The flight distance of this journey
-   Inflight wifi service: Satisfaction level of the inflight wifi
    service (0: Not Applicable; 1-5)
-   Departure/Arrival time convenient: Satisfaction level of
    Departure/Arrival time convenient
-   Ease of Online booking: Satisfaction level of online booking
-   Gate location: Satisfaction level of Gate location
-   Food and drink: Satisfaction level of Food and drink
-   Online boarding: Satisfaction level of online boarding
-   Seat comfort: Satisfaction level of Seat comfort
-   Inflight entertainment: Satisfaction level of inflight entertainment
-   On-board service: Satisfaction level of On-board service
-   Leg room service: Satisfaction level of Leg room service
-   Baggage handling: Satisfaction level of baggage handling
-   Check-in service: Satisfaction level of Check-in service
-   Inflight service: Satisfaction level of inflight service
-   Cleanliness: Satisfaction level of Cleanliness
-   Departure Delay in Minutes: Minutes delayed when departure
-   Arrival Delay in Minutes: Minutes delayed when Arrival
-   Satisfaction: Airline satisfaction level (Satisfaction, neutral or
    dissatisfaction)

The objective of our report is to predict passenger satisfaction with
airline services based on the provided dataset, which includes various
demographic and satisfaction-related variables such as gender, age,
travel type, flight class, and satisfaction levels with different
aspects of the journey. The dataset represents a survey on airline
passenger satisfaction and will be used to develop a predictive model to
determine whether passengers will be satisfied or dissatisfied with the
airline services.

Now we're going to get a summary of all the features in our dataset:

```{r}
summary(data)
```

Calculate the proportion of satisfied and dissatisfied customers in the
dataset.

```{r}
prop.table(table(data$satisfaction))
```

From the summary, it is evident that many features represent ratings on
the services provided by the airline agency, and these ratings range
from 0 to 5. Additionally, we noticed that the "Arrival Delay in
Minutes" feature contains some missing values (NA).

Next, we will examine the distribution of all nominal features.
Specifically, we have categorical data for Gender, Customer Type, Type
of Travel, and Class, while all the rating features are ordinal.

```{r}
table(data$Gender)
```

The "Gender" feature appears to be well-balanced, meaning that it has an
approximately equal number of occurrences for each category, likely male
and female. This balance can be beneficial for modeling as it prevents
any significant bias towards a particular gender in the analysis and
predictions.

```{r}
table(data$Customer.Type)
```

The "Customer Type" feature contains only two values, "disloyal
customer" and "loyal customer." The distribution of values is
imbalanced, with one category potentially having significantly more
occurrences than the other.

```{r}
table(data$Type.of.Travel)
```

The "Type of Travel" feature consists of only two values: "personal
travel" and "business travel." The distribution of values is imbalanced,
with "business travel" occurring twice as much as "personal travel."

```{r}
table(data$Class)
```

The "Class" feature contains three values: "business," "eco plus," and
"eco." The distribution of values is imbalanced. "Business" and "eco"
classes appear to be relatively balanced, while "eco plus" is
significantly underrepresented compared to the other two classes.

```{r}
table(data$satisfaction)
```

The "satisfaction" feature, which serves as our target variable, is an
important aspect of the analysis. The values for this feature are not
perfectly balanced, meaning that there is an unequal distribution of
satisfied and dissatisfied passengers in the dataset.

## Data preprocessing

In this section of data preprocessing, several steps are performed to
prepare the dataset for further analysis and modeling. The specific
actions taken include:

1.  Renaming columns: the names of the features (columns) are modified
    to improve their clarity and usability.

2.  Dropping unnecessary columns: two columns, "X" and "id," are removed
    from the dataset. The "X" column likely represents the index of the
    row, which does not carry any meaningful information for analysis.
    The "id" column is presumed to be an unknown indexing number, which
    may not contribute to the predictive modeling process.

3.  Converting categorical variables to factors: categorical variables,
    such as "Gender", "Customer Type", "Type of Travel" and "Class" are
    converted into factors.

By performing these data preprocessing steps, the dataset is cleaned and
transformed into a more suitable format for the subsequent analysis,
making it easier to build a predictive model for passenger satisfaction.

### Renaming columns and removing features

```{r}
# replace dots with underscores in column names
names(data) = gsub("\\.", "_", names(data))
# drop X and id column
data <- data %>% dplyr::select(-X, -id)
names(data)
```

### Convert categorical features to factors

```{r}
# convert categorical features to factor
data$Gender = as.factor(data$Gender)
data$Customer_Type = as.factor(data$Customer_Type)
data$Type_of_Travel = as.factor(data$Type_of_Travel)
data$Class = as.factor(data$Class)
data$satisfaction = as.factor(data$satisfaction)
data$Arrival_Delay_in_Minutes <- as.numeric(data$Arrival_Delay_in_Minutes)


ratings_fts_names = c("Inflight_wifi_service", "Departure_Arrival_time_convenient", 
  "Ease_of_Online_booking", "Gate_location", "Food_and_drink", "Online_boarding", 
  "Seat_comfort", "Inflight_entertainment", "On_board_service", "Leg_room_service", 
  "Baggage_handling", "Checkin_service", "Inflight_service", "Cleanliness", "On_board_service")

for (col in ratings_fts_names) {
  data[[col]] = as.factor(data[[col]])
}

```

## Handling na values

In this section, we analyze the dataset to identify variables with
missing values, particularly focusing on the "Arrival_Delay_in_Minutes"
variable. We calculate the proportion of missing values for this
variable and subsequently remove the examples or rows with missing
values from the dataset.

```{r}
# list features with na values
prop.table(colSums(is.na(data)))
```

To determine the proportion of missing values for the
"Arrival_Delay_in_Minutes" variable, we can count the number of
instances where this variable has missing values (commonly denoted as
"NaN" or "NA") and divide it by the total number of examples in the
dataset.

```{r}
# Arrival_Delay_in_Minutes has na values, proportion of na values
prop.table(table(is.na(data$Arrival_Delay_in_Minutes)))
```

Indeed, since the proportion of missing values for the
"Arrival_Delay_in_Minutes" variable is very low (less than 3% of the
entire dataset), it is reasonable to proceed with dropping these missing
values from the dataset.

```{r}
# drop na values in Arrival_Delay_in_Minutes
data = data[!is.na(data$Arrival_Delay_in_Minutes),]

any(is.na(data))
```

## Outliers

In this section, box plots are created for each numeric variable present
in the dataset. Box plots are a powerful visualization tool used to
identify the presence of outliers in the data. For each numeric
variable, the box plot displays a box that represents the interquartile
range (IQR), with the median indicated by a line inside the box. The
"lines" extending from the box show the range of the data, and any data
points beyond the lines are considered potential outliers.

```{r}

# plot boxplot of each numeric variable excluding ratings features
plots = list()
for (col in names(data)[sapply(data, is.numeric)]) {
  if (col %in% ratings_fts_names) {
    next
  }
  plot = ggplot(data, aes(x = .data[[col]])) +
  geom_boxplot() +
  labs(title = col, x = col, y = "Count") 
  plots[[col]] = plot
}

grid.arrange(grobs = plots, ncol = 2)
```

We can see that there are outliers in Departure_Delay_in_Minutes,
Arrival_Delay_in_Minutes and Flight_Distance. Considering the presence
of both near-zero and very large values in the dataset, alternative
distributions like the log-normal distribution may be more appropriate
for modeling the "Departure_Delay_in_Minutes" and
"Arrival_Delay_in_Minutes" variables, as they can better capture the
variability in delay times. Since we have decided to keep the outliers
in the dataset, it's essential to understand their potential impact on
our analysis and modeling. Outliers are data points that deviate
significantly from the rest of the data and can introduce noise or bias
in statistical analyses and machine learning models.

In the context of "Departure_Delay_in_Minutes,"
"Arrival_Delay_in_Minutes," and "Flight_Distance," outliers might
represent extreme values that could be caused by various factors, such
as severe weather conditions, operational disruptions, or exceptional
circumstances. By retaining these outliers, we acknowledge that such
extreme situations can occur and affect the overall flight delay and
distance patterns.

## Visualization

In this section, histograms are used to visualize the distribution of
the variables in the dataset, starting with the nominal features so that
we can gain insights into the distribution of categories within each
feature.

Upon visualizing the nominal features, it becomes apparent that some
features exhibit heavily unbalanced distributions. This means that
certain categories within these features have significantly higher
frequencies compared to others. The presence of such imbalanced
distributions could have implications for analysis and modeling, as it
may lead to biased results or difficulties in predicting less frequent
categories accurately.

```{r fig.height=8, fig.width=8}
# plot distribution of categorical variables
plots = list()
for (col in names(data)[sapply(data, is.factor)]) {
  if (col %in% ratings_fts_names) {
    next
  }
  plot = ggplot(data, aes(x = .data[[col]], fill = .data[[col]])) +
  geom_bar() +
  labs(title = paste("Histogram of", col), x = col, y = "Count") +
  guides(fill = FALSE)

  plots[[col]] = plot
}

grid.arrange(grobs = plots, ncol = 2)
```

From the analysis of the nominal features, we can observe the following
regarding their balance:

1.  Gender: it is almost perfectly balanced, meaning that there is a
    relatively equal representation of both genders in the dataset.

2.  Satisfaction: the target feature appears to be slightly imbalanced,
    with fewer instances of "satisfied" compared to the other class
    ("dissatisfied").

3.  Type of Travel: it shows an imbalance, with more instances of
    "business travel" compared to "personal travel." We can assume that
    the majority of people in this dataset travel for business.

4.  Class: it also exhibits imbalance, with "business" and "eco" classes
    having relatively balanced representations, while the "Eco Plus"
    class is underrepresented.

5.  Customer Type: it shows an imbalance, with a higher number of "loyal
    customer" instances compared to "disloyal customer."

When dealing with imbalanced data, we need to take specific measures
during model training and evaluation to ensure that the model performs
well and doesn't exhibit bias towards the majority class. Appropriate
techniques such as resampling, using different evaluation metrics or
employing specialized algorithms can help address the imbalance and lead
to a more accurate and fair predictive model.

Now we plot the distribution of ratings features.

```{r fig.height=18, fig.width=12}
# plot distribution of ratings features
plots = list()
my_palette <- c("#1f78b4", "#33a02c", "#e31a1c", "#ff7f00", "#6a3d9a", "#b15928")

for (col in names(data)[sapply(data, is.factor)]) {
  if (!col %in% ratings_fts_names) {
    next
  }
  plot <- ggplot(data, aes(x = .data[[col]], fill = factor(.data[[col]]))) +
    geom_bar() +
    geom_text(stat = 'count', aes(label = after_stat(count))) +  
    labs(title = paste("Histogram of ", col), x = col, y = "Count") +
    scale_fill_manual(values = my_palette) +
    guides(fill = FALSE)

  plots[[col]] <- plot
}
grid.arrange(grobs = plots, ncol = 3)
```

Based on the graphs showing the histograms of the ratings, we can
observe that the majority of them tend to fall between 3 and 4. This
conclusion is drawn from the visual representation of the data, where
the histogram bars are higher in the range of 3 to 4, indicating a
higher frequency of ratings in that range.

```{r}
# compute the mean value of all the ratings
ratings_data = data[, c(ratings_fts_names)]
ratings_data <- apply(ratings_data, 2, as.numeric)
ratings_mean = colMeans(ratings_data)
ratings_mean
```

This section presents histograms to visualize the distribution of
numeric variables in the dataset.

```{r fig.width=8}
# plot distribution and density of numeric variables excluding ratings features
plots = list()
for (col in names(data)[sapply(data, is.numeric)]) {
  if (col %in% ratings_fts_names) {
    next
  }
  plot = ggplot(data, aes(x = .data[[col]])) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = 0.5) +
  geom_density(alpha = 0.2, fill = "red") +
  labs(title = paste("Histogram of", col), x = col, y = "Count") 

  plots[[col]] = plot
}

grid.arrange(grobs = plots, ncol = 2)
```

The histograms provide a useful overview of the distribution of the
numeric variables in the dataset. This information can be used to
identify potential outliers, and to choose appropriate statistical
methods for analyzing the data:

1.  The age distribution is bimodal, which means that it has two
    distinct peaks. This could be due to a number of factors, such as
    the airline's target customer base, or the typical age of people who
    travel by air.

2.  The flight distance distribution is log-normal, which means that it
    is skewed to the left. This is likely due to the fact that there are
    a few very long flights, which skew the distribution.

3.  The departure delay and arrival delay distributions are very
    similar, which suggests that they are caused by the same factors.
    These factors could include weather conditions, air traffic control
    delays, or mechanical problems with the aircraft.

## Variables vs Target

### Categorical Variables vs Target

```{r fig.width=10}
# plots categorical variables vs satisfaction
plots = list()
for (col in names(data)[sapply(data, is.factor)]) {
  if (col == "satisfaction" || col %in% ratings_fts_names) {
    next
  }
  plot = ggplot(data, aes(x = satisfaction, fill = .data[[col]])) +
  theme_minimal() +
  geom_bar(position = "dodge") +
  labs(title = paste("Histogram of Satisfaction by", col), x = "Satisfaction", y = "Count")

  plots[[col]] = plot
  
}

grid.arrange(grobs = plots, ncol = 2)
```

The observations from the graph analysis provide valuable insights into
how different nominal features relate to passenger satisfaction:

1.  Gender: both males and females appear to have similar distributions
    in terms of satisfaction, indicating that gender may not be a strong
    predictor of passenger satisfaction. The distributions closely
    resemble the overall distribution of the satisfaction feature.

2.  Customer type: the graph suggests that "disloyal customers" are more
    likely to be unsatisfied or neutral compared to "loyal customers."
    This indicates that customer loyalty may play a role in passenger
    satisfaction, with loyal customers tending to be more satisfied.

3.  Type of travel: The graph indicates that "personal travelers" are
    more likely to be unsatisfied compared to "business travelers."
    Conversely, "business travelers" tend to have a higher proportion of
    satisfied passengers. This finding suggests that the purpose of
    travel may have an influence on passenger satisfaction.

4.  Class: The graph shows that "business class" passengers are more
    satisfied than unsatisfied, whereas "eco" and "eco plus" passengers
    tend to have a higher proportion of unsatisfied passengers. This
    suggests that the class of service provided by the airline may be a
    significant factor affecting passenger satisfaction.

### Rating Features vs Target

```{r fig.height=22, fig.width=10}
# plots ratings features vs satisfaction
plots = list()
for (col in names(data)[sapply(data, is.factor)]) {
  if (!col %in% ratings_fts_names) {
    next
  }
  plot = ggplot(data, aes(x = .data[[col]], fill = satisfaction)) +
  theme_minimal() +
  geom_bar(position = "dodge") +
  labs(title = paste("Histogram of Satisfaction by", col), x = "Satisfaction", y = "Count")

  plots[[col]] = plot  
}

grid.arrange(grobs = plots, ncol = 2)
```

Based on the observed distribution of the ratings, we can draw the
following conclusion:

1.  For most of the ratings, the mean value for unsatisfied/neutral
    consumers is around 3, except for "Inflight Service" and "Baggage
    Handling."

```{r}
# calculate the mean of the ratings of unsatisfied/neutral consumers
ratings_data = data[data$satisfaction == "neutral or dissatisfied", c(ratings_fts_names)]
ratings_data <- apply(ratings_data, 2, as.numeric)
ratings_mean = colMeans(ratings_data)
ratings_mean
```

2.  For most of the ratings given by satisfied consumers, the mean value
    is around 4. However, it's important to note that we cannot
    generalize from this information alone.

```{r}
# calculate the mean of the ratings of unsatisfied/neutral consumers
ratings_data = data[data$satisfaction == "satisfied", c(ratings_fts_names)]
ratings_data <- apply(ratings_data, 2, as.numeric)
ratings_mean = colMeans(ratings_data)
ratings_mean
```

3.  we can see that certain ratings, such as "Online_boarding,"
    "Seat_comfort," "Inflight_entertainment," "On_board_service," and
    "Leg_room_service," exhibit similar distributions. The
    neutral/unsatisfied votes appear to follow a normal curve centered
    around a mean of approximately 3, while the satisfied votes tend to
    concentrate on grades 4 and 5. This pattern strongly suggests that
    these ratings are not independent but rather correlated with each
    other.Through correlation matrix we'll have further considerations.

### Numerical Variables vs Target

```{r fig.height=7, fig.width=10}
# plots numeric variables vs satisfaction excluding ratings features with histograms of different colors for each satisfaction level
plots = list()
for (col in names(data)[sapply(data, is.numeric)]) {
  if (col %in% ratings_fts_names) {
    next
  }
  plot = ggplot(data, aes(x = .data[[col]], fill = satisfaction, group = satisfaction)) +
    theme_minimal() +
    geom_histogram(alpha = 0.5, bins = 30) +
    labs(title = paste("Histogram of", col), x = col, y = "Count") 

  plots[[col]] = plot
}

grid.arrange(grobs = plots, ncol = 2)

```

Based on the boxplots and the information provided, we can make the
following observations about the distributions of the numerical
features:

1.  Age: the boxplot for "Age" suggests that the distribution is
    approximately normal. Furthermore, it appears that neutral or
    dissatisfied customers tend to be slightly younger on average
    compared to satisfied customers.

2.  Flight Distance: the boxplot does not exhibit a normal distribution.
    Instead, it appears to have a right-skewed distribution. This is
    evident from the longer whisker on the right side, indicating that
    there are some outliers with larger flight distances.

3.  Departure Delay in Minutes: the boxplot for "Departure Delay in
    Minutes" also shows a right-skewed distribution. The majority of the
    data appears to be concentrated towards the lower values, with a lot
    of outliers representing longer departure delays.

4.  Arrival Delay in Minutes: similar to the "Departure Delay in
    Minutes," the boxplot for "Arrival Delay in Minutes" exhibits a
    right-skewed distribution. The bulk of the data is clustered towards
    the lower values, with a lot of outliers indicating longer arrival
    delays.

Based on these observations, it's essential to consider the appropriate
data transformations or use different distribution models when working
with "Flight Distance," "Departure Delay in Minutes," and "Arrival Delay
in Minutes." For example, logarithmic transformations may be suitable to
handle the skewed nature of these variables in statistical analyses and
modeling tasks.

## Correlation matrix

A correlation matrix is a table that shows how multiple variables in a
dataset are related to each other. It provides a simple and concise way
to see if there are any connections between the different variables.

In a correlation matrix, each row and column represent a variable, and
the cells display the correlation coefficients between pairs of
variables. The correlation coefficient is a value between -1 and 1 that
indicates the strength and direction of the relationship:

A correlation coefficient close to 1 suggests a strong positive
relationship (as one variable goes up, the other tends to go up as
well). A correlation coefficient close to -1 indicates a strong negative
relationship (as one variable goes up, the other tends to go down). A
correlation coefficient close to 0 suggests little to no linear
relationship between the variables. By examining the correlation matrix,
we can quickly identify which variables are positively correlated,
negatively correlated, or have no significant correlation. This helps us
understand how different factors in the dataset might be connected or
affect each other.

```{r}
## CORRELATION MATRIX FOR NUMERICAL VARIABLES

ds_cor1 <- cor(subset(data,select = c(
  Age,
  Flight_Distance,
  Departure_Delay_in_Minutes,
  Arrival_Delay_in_Minutes
)))

```

```{r fig.height=7, fig.width=10}
options(repr.plot.width = 14, repr.plot.height = 8)
corrplot(ds_cor1, na.label = " ", method="color", tl.col = "black", tl.cex = 1)
```

```{r}
ds_cor1_df <- data.frame()

# Loop through the rows and columns of 'ds_cor1'
for (row in rownames(ds_cor1)) {
  for (col in colnames(ds_cor1)) {
    # Check if the correlation is greater than 0.3 or less than -0.3
    if (ds_cor1[row, col] > 0.3 | ds_cor1[row, col] < -0.3) {
      # Create a new row with the feature names and correlation value
      new_row <- c(row, col, ds_cor1[row, col])
      # Append the new row to the 'ds_cor1_df'
      ds_cor1_df <- rbind(ds_cor1_df, new_row)
    }
  }
}

# Set appropriate column names for the result dataframe
colnames(ds_cor1_df) <- c("Feature1", "Feature2", "CorrelationValue")

# Delete correlation values for features with themselves
ds_cor1_df <- ds_cor1_df[ds_cor1_df$Feature1 != ds_cor1_df$Feature2, ]

# Remove duplicate rows based on the CorrelationValue column
ds_cor1_df <- ds_cor1_df[!duplicated(ds_cor1_df$CorrelationValue), ]

# Print the resulting dataframe
print(ds_cor1_df)
```

The correlation coefficient of 0.965 between
"Departure_Delay_in_Minutes" and "Arrival_Delay_in_Minutes" indicates a
strong positive relationship between these two variables. This suggests
that there is a high tendency for flights with longer departure delays
to also have longer arrival delays.

```{r}
## CORRELATION MATRIX FOR ORDINAL VARIABLES
ordinal_variables = c('Inflight_wifi_service', 'Departure_Arrival_time_convenient',
                      'Ease_of_Online_booking', 'Gate_location', 'Food_and_drink',
                      'Online_boarding', 'Seat_comfort', 'Inflight_entertainment',
                      'On_board_service', 'Leg_room_service', 'Baggage_handling',
                      'Checkin_service', 'Inflight_service','Cleanliness')

# for each ordinal variables, change the values to numeric
for (col in ordinal_variables) {
  data[[col]] = as.numeric(data[[col]])
}

ds_cor2 <- cor(subset(data, select = ordinal_variables))


```

```{r fig.height=7, fig.width=10}
options(repr.plot.width = 14, repr.plot.height = 8)
corrplot(ds_cor2, na.label=" ", tl.cex=1, tl.col="black", method="color")
```

```{r}
ds_cor2_df = data.frame()

for (row in rownames(ds_cor2)) {
  for (col in colnames(ds_cor2)) {
    # Check if the partial correlation is greater than 0.5 or less than -0.5
    if (ds_cor2[row, col] > 0.3 | ds_cor2[row, col] < -0.3) {
      # Create a new row with the feature names and correlation value
      new_row <- c(row, col, ds_cor2[row, col])
      # Append the new row to the 'partial_corr_cat_dataframe'
      ds_cor2_df <- rbind(ds_cor2_df, new_row)
    }
  }
}

# Set appropriate column names for the result dataframe
colnames(ds_cor2_df)= c("Feature1", "Feature2", "CorrelationValue")

# delete correlation value for feature with themselves
ds_cor2_df = ds_cor2_df[ds_cor2_df$Feature1 != ds_cor2_df$Feature2,]

ds_cor2_df <- ds_cor2_df[!duplicated(ds_cor2_df$CorrelationValue), ]
# Print the resulting dataframe
print(ds_cor2_df)
```

# Considerations

1.  Arrival Delay and Departure Delay: The high positive correlation
    between "Arrival Delay" and "Departure Delay" indicates that there
    is a strong relationship between these two variables. If a flight
    departs late, it is likely to arrive late as well. This correlation
    is not surprising, as delays in departure can cascade into delays
    throughout the flight's journey, affecting arrival time.

2.  Seat Comfort, Food and Drink, and In-Flight Entertainment: These
    variables show a positive correlation, suggesting that passengers
    who rate the seat comfort higher are also more likely to give
    positive ratings for food and drink quality and in-flight
    entertainment. This correlation aligns with common expectations
    since passenger satisfaction with various in-flight amenities tends
    to be interconnected.

3.  WiFi Service, Online Booking, and Gate Location: The positive
    correlation between "WiFi Service," "Online Booking," and "Gate
    Location" indicates that passengers who are satisfied with one of
    these factors are likely to be satisfied with the others as well.
    For instance, passengers who appreciate the convenience of online
    booking are more likely to expect good WiFi service and
    easy-to-access gate locations.

4.  Cleanliness with Gate Location, Food and Drink, Seat Comfort, and
    In-Flight Entertainment: The positive correlation between
    "Cleanliness" and several other variables, such as "Gate Location,"
    "Food and Drink," "Seat Comfort," and "In-Flight Entertainment,"
    implies that passengers who find these amenities satisfactory are
    also more likely to rate cleanliness positively. A clean and
    well-maintained environment often contributes to an overall positive
    flight experience, and passengers who enjoy other services are more
    likely to notice and appreciate cleanliness as well.

Understanding these correlations can help airlines and aviation
companies focus on improving the most critical aspects of the passenger
experience. For example, by addressing departure delays, airlines can
also reduce the likelihood of arrival delays and enhance customer
satisfaction. Additionally, enhancing seat comfort, in-flight
entertainment, and food and drink options together can lead to more
satisfied passengers.

## Partial correlation matrix

Partial correlations are used to explore and understand the direct
relationship between two variables while controlling for the influence
of other variables. In simpler terms, it helps us figure out the true
connection between two factors by removing the effects of other factors
that might be influencing them both Using partial correlations allows us
to dig deeper and find the direct links between variables, giving us a
more accurate understanding of their relationships in complex datasets.

```{r fig.height=7, fig.width=10}
numerical_features <- c("Age", "Flight_Distance", "Departure_Delay_in_Minutes", "Arrival_Delay_in_Minutes")
partial_correlations_num <- pcor(data[, numerical_features], method = "spearman")

# make the matrix
partial_corr_matrix_num <- partial_correlations_num$estimate

# Customize the plot size
options(repr.plot.width = 200, repr.plot.height = 200)

# Create the correlation plot
corrplot(partial_corr_matrix_num,
         method = "color",  # Choose "color" to visualize the correlations using colors
         type = "upper",    # Only plot the upper triangular part of the matrix (to avoid duplication)
         tl.col = "black",  # Label color
         tl.cex = 1,        # Label font size
)

#PARTIAL CORRELATION MATRIX FOR CATEGORICAL/ORDINAL
categorical_features = c('Inflight_wifi_service', 'Departure_Arrival_time_convenient',
                         'Ease_of_Online_booking', 'Gate_location', 'Food_and_drink',
                         'Online_boarding', 'Seat_comfort', 'Inflight_entertainment',
                         'On_board_service', 'Leg_room_service', 'Baggage_handling',
                         'Checkin_service', 'Inflight_service','Cleanliness')
partial_correlations_cat <- pcor(data[, categorical_features], method = "spearman")

# make the matrix
partial_corr_matrix_cat <- partial_correlations_cat$estimate

# Customize the plot size
options(repr.plot.width = 200, repr.plot.height = 200)

# Create the correlation plot
corrplot(partial_corr_matrix_cat,
         method = "color",  # Choose "color" to visualize the correlations using colors
         type = "upper",    # Only plot the upper triangular part of the matrix (to avoid duplication)
         tl.col = "black",  # Label color
         tl.cex = 1,        # Label font size
)



```

```{r}
partial_correlations_num_dataframe = as.data.frame(partial_correlations_num$estimate)

partial_corr_num_dataframe = data.frame()

for (row in rownames(partial_correlations_num_dataframe)) {
  for (col in colnames(partial_correlations_num_dataframe)) {
    # Check if the partial correlation is greater than 0.5 or less than -0.5
    if (partial_correlations_num_dataframe[row, col] > 0.5 | partial_correlations_num_dataframe[row, col] < -0.5) {
      # Create a new row with the feature names and correlation value
      new_row <- c(row, col, partial_correlations_num_dataframe[row, col])
      # Append the new row to the 'partial_corr_num_dataframe'
      partial_corr_num_dataframe <- rbind(partial_corr_num_dataframe, new_row)
    }
  }
}

# Set appropriate column names for the result dataframe
colnames(partial_corr_num_dataframe)= c("Feature1", "Feature2", "CorrelationValue")

# delete correlation value for feature with themselves
partial_corr_num_dataframe = partial_corr_num_dataframe[partial_corr_num_dataframe$Feature1 != partial_corr_num_dataframe$Feature2,]


partial_corr_num_dataframe <- partial_corr_num_dataframe[!duplicated(partial_corr_num_dataframe$CorrelationValue), ]

# Print the resulting dataframe
print(partial_corr_num_dataframe)

partial_correlations_cat_dataframe = as.data.frame(partial_correlations_cat$estimate)

partial_corr_cat_dataframe = data.frame()

for (row in rownames(partial_correlations_cat_dataframe)) {
  for (col in colnames(partial_correlations_cat_dataframe)) {
    # Check if the partial correlation is greater than 0.5 or less than -0.5
    if (partial_correlations_cat_dataframe[row, col] > 0.3 | partial_correlations_cat_dataframe[row, col] < -0.3) {
      # Create a new row with the feature names and correlation value
      new_row <- c(row, col, partial_correlations_cat_dataframe[row, col])
      # Append the new row to the 'partial_corr_cat_dataframe'
      partial_corr_cat_dataframe <- rbind(partial_corr_cat_dataframe, new_row)
    }
  }
}

# Set appropriate column names for the result dataframe
colnames(partial_corr_cat_dataframe)= c("Feature1", "Feature2", "CorrelationValue")

# delete correlation value for feature with themselves
partial_corr_cat_dataframe = partial_corr_cat_dataframe[partial_corr_cat_dataframe$Feature1 != partial_corr_cat_dataframe$Feature2,]

partial_corr_cat_dataframe <- partial_corr_cat_dataframe[!duplicated(partial_corr_cat_dataframe$CorrelationValue), ]
# Print the resulting dataframe
print(partial_corr_cat_dataframe)

```

We computed the partial correlations for each pair of categorical
features and identified correlations with an absolute value greater than
0.5. We then removed rows where Feature1 and Feature2 are equal and
removed any duplicate correlations to obtain the final results.

Below are the significant partial correlations between categorical
features:

1.  Inflight WiFi service and Ease of Online booking: 0.552 There is a
    moderate positive correlation between passengers' satisfaction with
    inflight WiFi service and the ease of the online booking process.

2.  Food and drink and Inflight entertainment: 0.349 Passengers who are
    satisfied with the food and drink options onboard are more likely to
    enjoy the inflight entertainment.

3.  Online boarding and Seat comfort: 0.318 There is a positive
    correlation between passengers' satisfaction with the online
    boarding process and their comfort with the seating arrangements.

4.  Inflight entertainment and Cleanliness: 0.401. Passengers who find
    the inflight entertainment enjoyable are more likely to rate the
    cleanliness of the aircraft positively.

5.  Baggage handling and Inflight service: 0.372 There is a positive
    correlation between passengers' satisfaction with baggage handling
    and their perception of inflight service quality.

For numerical features, we have only a significant partial correlation
between departure_Delay_in_Minutes and arrival_Delay_in_Minutes. It was
expected and it can confirm our assumption: if a flight has a departure
delay, it's very likely to have also an arrival delay.

## Convert categorical to numerical

In this section we convert the categorical variables to numeric
representation for modelling.

```{r}
gender_map = c("Male" = 0, "Female" = 1)
data$Gender = gender_map[as.numeric(data$Gender)]

customer_type_map = c("Loyal Customer" = 0, "disloyal Customer" = 1)
data$Customer_Type = customer_type_map[as.numeric(data$Customer_Type)]

type_of_travel_map = c("Personal Travel" = 0, "Business travel" = 1)
data$Type_of_Travel = type_of_travel_map[as.numeric(data$Type_of_Travel)]

class_map = c("Business" = 0, "Eco" = 1, "Eco Plus" = 2)
data$Class = class_map[as.numeric(data$Class)]

satisfaction_map = c("neutral or dissatisfied" = 0, "satisfied" = 1)
data$satisfaction = satisfaction_map[as.numeric(data$satisfaction)]

for (col in ratings_fts_names) {
  data[[col]] <- as.numeric(data[[col]])
}
```

# Data Preparation

## Train test split

This section splits the data into training and testing sets, prints the
proportion of satisfied and dissatisfied customers in each set, and
saves the true values of the target variable for the test set.

```{r}
set.seed(123)

train_index = sample(1:nrow(data), 0.8*nrow(data))
# 80% of data is used for training
train_data = data[train_index,]
# 20% of data is used for testing
test_data = data[-train_index,]

```

## Features and Outputs

```{r}

# Seperate y_train and y_test for further use
X_train = as.matrix(train_data %>% dplyr::select(-satisfaction))
y_train = train_data$satisfaction

X_test = as.matrix(test_data %>% dplyr::select(-satisfaction))
y_test = test_data$satisfaction
```

## Number of Samples

```{r}
# Number of samples in train data
train_rows <- nrow(train_data)
print(train_rows)

# Number of samples in test data
test_rows <- nrow(test_data)
print(test_rows)
```

## Data Balance

```{r}
# Proportion of satisfied and unsatisfied customers for train data
prop.table(table(y_train))
# Proportion of satisfied and unsatisfied customers for test data
prop.table(table(y_test))
```

As we can see the proportion of binary classes in train and test are
similar. We can say that our test data is representative of train data.

In this project we want to find unsatisfied customers (class 0). We
hypothesis that if we find the unsatisfied customers, then we arrange
our customer satisfaction campaign accordingly.

# Classification Models

Classification, a form of supervised learning, involves predicting the
qualitative response of an observation by assigning it to a specific
category or class. Multivariate techniques excel in constructing
prediction models, forming the foundation of statistical classification.
In the realm of machine learning, this process is referred to as
supervised learning.

To build a classifier, we utilize a set of training observations. From a
geometric perspective, an allocation rule determines how the input space
is divided into regions, each labeled according to its classification.
The boundaries of these regions can vary based on the assumptions made
when constructing the classification model.

## Logistic Regression

In logistic model observations are assumed to be realizations of
independent Bernoulli random variables. It is like if a customer will be
satisfied with flight or not.

### Basic Logistic Classifier

```{r}
# Model definition with all features:
glm_full<- glm(data = train_data,satisfaction ~ .,
                family = "binomial")
# summary of full model
summary(glm_full)
```

As we can see from model statistics, the p value of "flight distance"
feature is not lower than 0.5 which indicates that the effect of "flight
distance" on prediction of satisfaction is insignificant. However, we
would like to keep all features in the model for now. Then we will full
model with feature selection models.

In the code snippet below, we can observe the calculation goodness of
fit, also known as R-squared (R²). R² serves as a metric to assess how
well the GLM (Generalized Linear Model) model aligns with the given data
in comparison to a null model, which solely consists of an intercept
without any predictors. The R² value is derived using a simple formula:
1 minus the ratio of the deviance of the fitted model to the deviance of
the null model.

R² value provides insights into the amount of variation in the data that
can be accounted for by the model, thus serving as a measure of its
effectiveness.

```{r}
r2<- 1 - (summary(glm_full)$deviance/summary(glm_full)$null.deviance)
r2
```

In the provided line of code, the calculation of the variance inflation
factor (VIF) is performed by taking the reciprocal of 1 minus the R²
value. The VIF serves as a metric to evaluate the presence of
multicollinearity, which refers to the correlation between predictor
variables within a model. High levels of multicollinearity can introduce
issues regarding the reliability and interpretability of the GLM
(Generalized Linear Model).

```{r}
1/(1-r2)
```

### Logistic Regression with Backward Variable Selection

Collinearity refers to a situation where two or more predictor variables
in a statistical model are closely related to each other. The presence
of collinearity can introduce instability to the model and make it more
challenging to accurately estimate the effect of each predictor
variable.

Ideally, we aim for low collinearity among predictor variables. When
collinearity is low, it implies that the predictor variables are
independent or weakly correlated, allowing for more reliable estimation
of their individual effects on the model.

The Variance Inflation Factor (VIF) is a measure used to assess
collinearity among predictor variables within a multiple regression
model. It is computed by taking the ratio of the variance of all the
weights in the model divided by the variance of a single weight if that
particular predictor variable were fitted alone.

A VIF of 1 indicates no collinearity, while a VIF between 1 and 5
suggests moderate collinearity. On the other hand, a VIF greater than 5
indicates high collinearity, which is undesirable and should be avoided.

To address high collinearity, we may choose to remove certain predictor
variables from the model and observe the impact on the corresponding
R-squared value. By iteratively adjusting the predictor variables, we
can determine a suitable set of variables that minimize collinearity and
yield a more robust model.

Variable selection is crucial in regression models to ensure
interpretability, computational efficiency, and generalization. It
involves removing irrelevant variables, reducing redundancy, and
combating overfitting. By choosing relevant variables, the model becomes
easier to interpret, algorithms work faster with high-dimensional data,
and overfitting is reduced. The focus is on low test error rather than
training error. Variable selection is especially important for
high-dimensional datasets with more features than observations. Here, we
are gonna use backward selection starting from full model and reducing
number of variables in order of high VIF factor to lower VIF factors to
decrease multi-collinearity and increase robustness of model.

```{r}

# VIF Iteration 0
# The process is done iteratively where we delete one variable at time
vif_values <- VIF(glm_full)

# Create a data frame with variable names and their corresponding VIF values
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values,row.names = NULL)

# Sort the data frame in decreasing order of VIF values
sorted_df <- vif_df[order(-vif_df$VIF), ]
rownames(sorted_df) <- NULL

# Print the sorted data frame
print(sorted_df)
```

```{r}
# VIF Iteration 1
# The process is done iteratively where we delete one variable at time
# Model definition:
glm_vif1<- glm(data = train_data,
                satisfaction ~ .-Arrival_Delay_in_Minutes,
                family = "binomial")
vif_values <- VIF(glm_vif1)

# Create a data frame with variable names and their corresponding VIF values
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values,row.names = NULL)

# Sort the data frame in decreasing order of VIF values
sorted_df <- vif_df[order(-vif_df$VIF), ]
rownames(sorted_df) <- NULL

# Print the sorted data frame
print(sorted_df)
```

```{r}
# VIF Iteration 2
# The process is done iteratively where we delete one variable at time
# Model definition:
glm_vif2<- glm(data = train_data,
                satisfaction ~ .
               -Arrival_Delay_in_Minutes
               -Inflight_entertainment,
                family = "binomial")
vif_values <- VIF(glm_vif2)

# Create a data frame with variable names and their corresponding VIF values
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values,row.names = NULL)

# Sort the data frame in decreasing order of VIF values
sorted_df <- vif_df[order(-vif_df$VIF), ]
rownames(sorted_df) <- NULL

# Print the sorted data frame
print(sorted_df)
```

```{r}
# VIF Iteration 3
# The process is done iteratively where we delete one variable at time
# Model definition:
glm_vif3<- glm(data = train_data,
                satisfaction ~ .
                -Arrival_Delay_in_Minutes
                -Inflight_entertainment
                -Ease_of_Online_booking	,
                family = "binomial")
vif_values <- VIF(glm_vif3)

# Create a data frame with variable names and their corresponding VIF values
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values,row.names = NULL)

# Sort the data frame in decreasing order of VIF values
sorted_df <- vif_df[order(-vif_df$VIF), ]
rownames(sorted_df) <- NULL

# Print the sorted data frame
print(sorted_df)
```

```{r}
# VIF Iteration 4
# The process is done iteratively where we delete one variable at time
# Model definition:
glm_vif4<- glm(data = train_data,
                satisfaction ~ .
               -Arrival_Delay_in_Minutes
               -Inflight_entertainment
               -Ease_of_Online_booking 
               -Cleanliness	,
                family = "binomial")
vif_values <- VIF(glm_vif4)

# Create a data frame with variable names and their corresponding VIF values
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values,row.names = NULL)

# Sort the data frame in decreasing order of VIF values
sorted_df <- vif_df[order(-vif_df$VIF), ]
rownames(sorted_df) <- NULL

# Print the sorted data frame
print(sorted_df)
```

We deleted all the features with VIF greater than 2 to decrease
multicollinearity. Let's check the current model statistics.

```{r}
glm_reduced<- glm(data = train_data,
                satisfaction ~ .
                -Arrival_Delay_in_Minutes
                -Inflight_entertainment
                -Ease_of_Online_booking 
                -Cleanliness	,
                family = "binomial")
# Observation of the model summary:
summary(glm_reduced)
```

"Flight_Distance" and "Gate_location" features are insignificant in
prediction of satisfaction. Let's drop them from model.

```{r}
# Drop Flight_Distance and Gate_location from model.

glm_backward<- glm(data = train_data,
                satisfaction ~ .
                -Arrival_Delay_in_Minutes
                -Inflight_entertainment
                -Ease_of_Online_booking 
                -Cleanliness
                -Flight_Distance
                -Gate_location,
                family = "binomial")

summary(glm_backward)
```

Interestingly, model with backward elimination variable selection has
less R² value than full model. We will discuss the reason at the end of
Logistic Regression part.

```{r}
r2<- 1 - (summary(glm_backward)$deviance/summary(glm_backward)$null.deviance)
r2
```

### Logistic Regression with Shrinkage Method

Shrinkage methods, such as Ridge and Lasso regression, are techniques
used in linear modeling to control model complexity and reduce
overfitting. Instead of selecting a subset of predictors or setting some
coefficients to zero, these methods constrain or regularize the
coefficient estimates, effectively shrinking them towards zero. Ridge
regression achieves this by using quadratic shrinking, while Lasso
regression uses absolute-value shrinking. Other hybrid approaches, like
the elastic net, combine elements of both methods.

One disadvantage of ridge regression is that it includes all predictors
in the final model, unlike subset selection methods which typically
select a subset of variables. To overcome this drawback, the lasso
regression is used as an alternative. The lasso also shrinks the
coefficient estimates towards zero, and it tends to perform better.

Lambda is the Tuning Parameter that controls the bias-variance tradeoff
and we estimate its best value via cross-validation.

```{r}
# We look for the best value for lambda
# We use cross validation glmnet
glm_lasso <- cv.glmnet(X_train, as.factor(y_train),
                      alpha = 0, family = "binomial", type.measure = "class")
plot(glm_lasso)

```

```{r}
# We identify th best lambda value
best_lambda <- glm_lasso$lambda.min
best_lambda
```

### ROC Curve

We created our 3 different logistic regression model. Now, it is time to
compare them on test sets to see their robustness on generalization and
power on predicting satisfaction of customer.

```{r}

# Full model
# Computing the predictions with the model on the test set:
pred_glm_full<- predict(glm_full, data.frame(X_test), type = "response")

# Backward elimination selection
# Computing the predictions with the model on the test set:
pred_glm_backward<- predict(glm_backward, data.frame(X_test), type = "response")

# Lasso regresssion
# Computing the predictions with the model on the test set:
pred_glm_lasso<- predict(glm_lasso, X_test, type= "response", s = best_lambda)

```

The Receiver Operating Characteristics (ROC) curve illustrates the
relationship between the True positive rate and the False positive rate
across various thresholds. In an ideal scenario, the ROC curve would
closely follow the top left corner. The overall effectiveness of a
classifier, considering all thresholds, is quantified by the Area Under
the Curve (AUC). A larger AUC value indicates a superior classifier
performance.

```{r roc, echo=FALSE, message=FALSE, warnings=FALSE}
par(pty="s")
roc(y_test,pred_glm_full,plot=TRUE, legacy.axes=TRUE, percent=TRUE, 
    xlab="False Positive Percentage", ylab="True Positive Percentage",
    col="blue", lwd=4,
    print.auc=TRUE, print.auc.y=60, print.auc.x=30,
    quiet = TRUE)

plot.roc(y_test,pred_glm_backward,add=TRUE, legacy.axes=TRUE, percent=TRUE, 
    xlab="False Positive Percentage", ylab="True Positive Percentage",
    col="red", lwd=4,
    print.auc=TRUE, print.auc.y=50, print.auc.x=30,
    quiet = TRUE)

plot.roc(y_test,pred_glm_lasso,add=TRUE, legacy.axes=TRUE, percent=TRUE, 
    xlab="False Positive Percentage", ylab="True Positive Percentage",
    col="green", lwd=4,
    print.auc=TRUE, print.auc.y=40, print.auc.x=30,
    quiet = TRUE)

legend("bottomright",
       legend=c("glm_full","glm_backward","glm_lasso"),
       col=c("blue","red","green"),
       lwd=4)
```

Ideally, we expect backward and lasso model to have better
generaliation; however, in this scenario, the results yield opposite
situation.

### Comparison of Logistic Classifiers

We have 3 different Logistic Regression models 1- Basic Logistic
Classifier : glm_full 2- Logistic Regression with Backward Variable
Selection: glm_backward 3- Logistic Regression with Lasso Shrinkage:
glm_lasso

Now, it is time to make predictions and compare metric results. But,
first we should decide best thresholds for models.

```{r}
# This function will return evaluation metrics
calculate_evaluation_metrics <- function(thresholds, output_list, y_test) {
  # Create an empty data frame to store the results
  results_df <- data.frame(
    Threshold = numeric(length(thresholds)),
    Accuracy = numeric(length(thresholds)),
    F1_Score = numeric(length(thresholds)),
    Precision = numeric(length(thresholds)),
    Recall = numeric(length(thresholds))
  )
  
  # Calculate evaluation metrics for each threshold 
  # Store the results in the data frame
  for (i in 1:length(thresholds)) {
    threshold <- thresholds[i]
    
    pred_output <- output_list[[as.character(threshold)]]
    
    results_df[i, "Threshold"] <- threshold
    results_df[i, "Accuracy"] <- Accuracy(y_pred = pred_output, y_true = y_test)
    results_df[i, "F1_Score"] <- F1_Score(y_pred = pred_output, y_true = y_test)
    results_df[i, "Precision"] <- Precision(y_pred = pred_output, y_true = y_test)
    results_df[i, "Recall"] <- Recall(y_pred = pred_output, y_true = y_test)
  }
  
  # Format the floating-point numbers with two decimal places
  results_df$Accuracy <- round(results_df$Accuracy, 4)
  results_df$F1_Score <- round(results_df$F1_Score, 4)
  results_df$Precision <- round(results_df$Precision, 4)
  results_df$Recall <- round(results_df$Recall, 4)
  
  return(results_df)
}

```

```{r}
evaluate_on_thresholds <- function(predictions,y_test, thresholds) {
  # Converting the prediction in {0,1} according to the chosen threshold:
  output_list <- list()
  
  for (threshold in thresholds) {
    output <- ifelse(predictions > threshold, 1, 0)
    output_list[[as.character(threshold)]] <- output
  }
  
  # Access the outputs using the threshold values as keys
  #output_list$`0.4`
  #output_list$`0.5`
  #output_list$`0.6`
  #output_list$`0.7`
  
  
  # Calculate evaluation metrics
  results <- calculate_evaluation_metrics(thresholds, output_list, y_test)
  
  # Print the results as a table in R Markdown
  knitr::kable(results, align = "c")
}
```

```{r}
thresholds <- c(0.4, 0.5, 0.6, 0.7)
evaluate_on_thresholds(pred_glm_full, y_test, thresholds)
```

```{r}
thresholds <- c(0.4, 0.5, 0.6, 0.7)
evaluate_on_thresholds(pred_glm_backward, y_test, thresholds)
```

```{r}
thresholds <- c(0.4, 0.5, 0.6, 0.7)
evaluate_on_thresholds(pred_glm_lasso, y_test, thresholds)
```

All 3 logistic regression models have highest F1 score with 0.6
threshold. Eventhough the expectation that backward variable selection
and lasso regression model would suprass full_model, the results shows
opposite. F1 score with 0.6 threshold for glm_full is higher than other
models on the test set. We can conclude that generalizability of full
model is higher. We may conclude that we have many samples but we do not
have enough features to increase model generalizability, so decreasing
number of variables or shrinking their weights do not make positive
effect. We take pred_glm_full as a winner prediction from logistic
regression part to further compare it with other models.

## Naive Bayes

The Naive Bayes Classifier is a probabilistic algorithm used for binary
classification. It assumes that features are independent of each other
and calculates prior probabilities and likelihoods during the training
phase. The prior probabilities represent the occurrence of each class,
while the likelihoods determine the probability of a feature given a
class. By applying Bayes' theorem, the algorithm calculates posterior
probabilities for each class and assigns the instance to the class with
the highest probability.

```{r}
nb.fit <- naiveBayes(data = data.frame(X_train),
                     y_train ~ .)

# Make predictions on the test data
pred_naive_bayes <- predict(nb.fit, newdata = X_test)

# Evaluate accuracy of classifier
mean(pred_naive_bayes == y_test)
```

BIC (Bayesian Information Criterion) is a commonly used model selection
criteria that help in selecting the best model among a set of competing
models. It takes into account the goodness of fit of the model and
penalize the complexity of the model to avoid overfitting.

BIC (Bayesian Information Criterion): It balances the trade-off between
model fit and model complexity. Formula: BIC = -2 \* log-likelihood + p
\* log(n)

-   log-likelihood: The log-likelihood of the model, which measures how
    well the model fits the data.
-   p: The number of parameters in the model.
-   n: The sample size.

BIC penalizes model complexity more heavily than the Akaike Information
Criterion (AIC). The lower the BIC value, the better the model is
considered to be. Therefore, when comparing models, the model with the
lowest BIC is preferred.

```{r}

# Best Subset Selection

# The regsubsets() function (part of the leaps library) 
# performs best subset selection by identifying the best 
# model that contains a given number of predictors, 
# where best is quantified using bic

n <- dim(X_train)[1]
regfit.full <- regsubsets(y_train~.,data=data.frame(X_train),nvmax=n)
reg.summary <- summary(regfit.full)

# Plotting BIC 
# BIC with its smallest value
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
min <- which.min(reg.summary$bic)
points(10,reg.summary$bic[10],col="red",cex=2,pch=20)
```

Since BIC (Bayesian Information Criterion) does not change dramatically
after 10th variable. We can use only 10 variables to decide
satisfaction.

```{r}
# choose 10 variable model
best_model <- coef(regfit.full, id = 10) 
abs_coefficients <- abs(best_model)
sorted_variables <- names(sort(abs_coefficients, decreasing = TRUE))
column_names <- sorted_variables[-1]
new_X_train <- X_train[, column_names, drop = FALSE]
new_X_test <- X_test[, column_names, drop = FALSE]

nb.fit <- naiveBayes(data = data.frame(new_X_train),
                     y_train ~ .)

# Make predictions on the test data
pred_naive_bayes <- predict(nb.fit, newdata = new_X_test)

# Evaluate accuracy
mean(pred_naive_bayes == y_test)
```

The accuracy of model with 10 variables is higher. We can continue with
this model.

## K-Nearest Neigbors (KNN)

K-Nearest Neighbors (KNN) is a supervised machine learning algorithm
used for classification and regression tasks. It is a non-parametric
algorithm, which means it doesn't make any assumptions about the
underlying distribution of the data. KNN is a simple yet powerful
algorithm that is widely used for its intuitive concept and easy
implementation.

The main idea behind KNN is to classify a new data point based on the
majority vote of its neighbors. The algorithm assumes that similar data
points tend to belong to the same class or have similar numerical
values. The "K" in KNN refers to the number of nearest neighbors that
are considered for classification or regression.

In this model, we used K-fold cross validation. Cross-validation is a
valuable technique in statistics and machine learning that helps assess
the performance of a model and select optimal hyperparameters.
Specifically, the K-fold cross-validation method is commonly employed
for this purpose.

During K-fold cross-validation, the training set is divided into K
equally sized subsets or "folds." The model is then trained K times,
each time using K-1 of the folds as the training data and leaving one
fold as the validation set. This process is repeated K times, ensuring
that each fold serves as the validation set exactly once.

We can mention two benefits of k-fold cross validation. First, it allows
us to leverage the entire training dataset for both training and
validation. By iteratively rotating the folds, every data point gets an
opportunity to be in the validation set, providing a more comprehensive
evaluation of the model's performance.

Secondly, K-fold cross-validation helps to mitigate the potential bias
introduced by using a single validation set. When we reserve a separate
validation set, there is a risk that the performance estimation becomes
overly influenced by the specific data points in that set. By repeatedly
shuffling and partitioning the data into different folds, we obtain a
more robust estimate of the model's performance, as it is evaluated on
multiple distinct subsets of the data.

The final evaluation of the model is typically based on the average
performance across all K iterations. This averaging process helps to
reduce the variance in the performance estimate and provides a more
stable measure of the model's effectiveness.

We need to scale our data for two reasons. Firstly, scaling increases
the speed of training process. Secondly, if we do not scale the data,
the features with higher value will have more impact on predictions,
however, the features should have same weight and the change only should
be in range of 0-1.

```{r}

# Function for feature scaling
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
# We normalize the columns
train_scaled <- as.data.frame(lapply(train_data, min_max_norm))
test_scaled<- as.data.frame(lapply(test_data, min_max_norm))
```

```{r knn}
# KNN with K-fold cross validation

# Define a range of K values
k_values <- 1:10

# Perform cross-validation and calculate error rates
error_rates <- sapply(k_values, function(k) {
  set.seed(123)  # For reproducibility
  model <- train(as.factor(satisfaction)~., data = train_scaled, 
                 method = "knn", 
                 trControl = trainControl(method = "cv", number = 5),
                 tuneGrid = data.frame(k = k))
  1 - model$results$Accuracy
})

# Plot the Error Rates
plot(k_values, error_rates, type = "b", pch = 16, xlab = "K Value", ylab = "Error Rate",
     main = "KNN: Error Rate vs. K Value")


```

```{r}
# find the k giving minimum error rate
k_min <- which.min(error_rates)
k_min
```

```{r}
# make predictions with k = k_min 
pred_knn<- knn(train_scaled[,-23], test_scaled[,-23],
                cl = train_scaled$satisfaction, 
                k = k_min)
```

# Classification Results

## Confusion Matrix and Metrics

```{r}
# Create a function for confusion matrix and other metrics
draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Unsatisfied', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Satisfied', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Unsatisfied', cex=1.2, srt=90)
  text(140, 335, 'Satisfied', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(50, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(30, 40, names(cm$byClass[5]), cex=1.2, font=2)
  text(30, 30, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(50, 40, names(cm$byClass[7]), cex=1.2, font=2)
  text(50, 30, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  text(70, 40, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 30, round(as.numeric(cm$byClass[6]), 3), cex=1.2)


}  
```

```{r}
# Confusion matrix for glm_full_model

# Use best threshold for prediction (0 or 1)
Threshold <- 0.6
pred_glm_full_factor <- as.factor(ifelse(pred_glm_full >= Threshold , 1,0))


conf_matrix_glm <- confusionMatrix(data = pred_glm_full_factor, 
                                     reference = as.factor(y_test))
draw_confusion_matrix(conf_matrix_glm)
```

```{r}
# Confusion matrix for Naive Bayes
conf_matrix_naive <- confusionMatrix(data = pred_naive_bayes, 
                                     reference = as.factor(y_test))
draw_confusion_matrix(conf_matrix_naive)
```

```{r}
# Confusion matrix for KNN
conf_matrix_knn <- confusionMatrix(data = pred_knn, reference = as.factor(y_test))
draw_confusion_matrix(conf_matrix_knn)
```

# Conclusions

In our project, we want to find unsatisfied customers as precise as
possible. Because, our hypothesis is to increase total customer
satisfaction with low budget. Therefore, finding unsatisfied customers
and having less satisfied customer in our target will be the best
method. Considering that, precision metric will be most valuable metric.
Then we can count on F1 score, since it considers both Precision and
Recall into account.

We explored three different classification algorithms: Logistic
Regression with different variable selection techniques, Naive Bayes,
and K-Nearest Neighbors (KNN), to compare their performance.

Logistic Regression with the full features model demonstrated the best
generalizability among all the tested models. Despite the expectation
that backward variable selection and lasso regression might improve
performance, the full features model outperformed them on the test set.

Naive Bayes showed promising results with a respectable accuracy score.
However, it had slightly lower precision and F1 score compared to the
best-performing model, Logistic Regression with the full features model.

KNN has the highest precision score rather than Naive Bayes and Logistic
Regression with full features model. However, KNN is an non-parametric
model and it does not have any bias related to hypothesis space.
Therefore, the complexity of model increases with number of samples.
Besides, we used cross-validation which also increases model complexity.
We can use a validation set approach to decrease training time.

Considering the main objective of this project to maximize precision, we
select the best-performing model: Logistic Regression with the full
features model. This model demonstrated superior performance in terms of
precision, recall, and F1 score. Moreover, its generalizability suggests
that it can be effectively applied to new and unseen data.

Final Remarks Overall, the results of this project highlight the
potential of machine learning in predicting customer satisfaction. The
selected Logistic Regression model with full features showcases
promising performance and generalizability. By identifying and
addressing the pain points of unsatisfied customers, businesses can take
proactive measures to enhance customer satisfaction and loyalty,
ultimately leading to higher customer retention and overall business
success.
